{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxNCGz6UXAfO"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!git clone https://github.com/IntelLabs/FP8-Emulation-Toolkit.git\n",
        "%cd FP8-Emulation-Toolkit\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn1rm5mraSqZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/FP8-Emulation-Toolkit\n",
        "!python setup.py clean --all\n",
        "!python setup.py build_ext --inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N56XVH6abGe"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P2ahKDnacD1",
        "outputId": "4d22942d-5cda-428c-f83c-00bfadf2b09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP8 Emulation Toolkit installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import mpemu\n",
        "print(\"FP8 Emulation Toolkit installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from Conv2d import CustomConv2D  # Import custom convolution layer\n",
        "from mpemu import mpt_emu  # Import FP8 Emulation Toolkit\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, custom_conv=False):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        if custom_conv:\n",
        "            # Use custom conv layer for inference\n",
        "            self.conv1 = CustomConv2D(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = CustomConv2D(32, 64, kernel_size=3, padding=1)\n",
        "        else:\n",
        "            # Use PyTorch's Conv2d for training\n",
        "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model for training (using PyTorch's Conv2d)\n",
        "model = MNISTModel(custom_conv=False).float()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, train_loader, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "train_model(model, criterion, optimizer, train_loader, device, num_epochs)\n",
        "\n",
        "# switch to custom conv layers for inference\n",
        "model_custom = MNISTModel(custom_conv=True)\n",
        "model_custom.load_state_dict(model.state_dict())\n",
        "model_custom = model_custom.to(device)\n",
        "\n",
        "# Quantize the model for inference using FP8 (after training is complete)\n",
        "list_exempt_layers = [\"conv1\",\"conv2\",\"fc1\", \"fc2\"]  # Exempt fully connected layers from FP8 conversion\n",
        "model_custom, emulator = mpt_emu.quantize_model(model_custom, dtype=\"E4M3\", list_exempt_layers=list_exempt_layers)\n",
        "\n",
        "# Function to evaluate the model after quantization\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy after quantization: {accuracy:.2f}%')\n",
        "\n",
        "# Evaluate the quantized model on the test set\n",
        "evaluate_model(model_custom, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr59cQkXdreC",
        "outputId": "932b075f-67dd-4a98-d440-fb8eaeecd76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1362\n",
            "Epoch [2/5], Loss: 0.0427\n",
            "Epoch [3/5], Loss: 0.0283\n",
            "Epoch [4/5], Loss: 0.0216\n",
            "Epoch [5/5], Loss: 0.0168\n",
            "e4m3 : quantizing model weights..\n",
            "Accuracy after quantization: 98.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from Conv2d import CustomConv2D\n",
        "from mpemu import mpt_emu\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, custom_conv=False):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        if custom_conv:\n",
        "            # Use custom conv layer for inference\n",
        "            self.conv1 = CustomConv2D(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = CustomConv2D(32, 64, kernel_size=3, padding=1)\n",
        "        else:\n",
        "            # Use PyTorch's Conv2d for training\n",
        "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to print the input and output tensors of each layer\n",
        "def hook_fn(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        print(f\"Layer: {layer_name}\")\n",
        "        print(f\"Input tensor shape: {input[0].shape}\")\n",
        "        print(f\"Output tensor shape: {output.shape}\")\n",
        "        print(f\"Input tensor: {input[0]}\")\n",
        "        print(f\"Output tensor: {output}\\n\")\n",
        "    return hook\n",
        "\n",
        "# Attach hooks to layers\n",
        "def attach_hooks(model):\n",
        "    model.conv1.register_forward_hook(hook_fn('conv1'))\n",
        "    model.conv2.register_forward_hook(hook_fn('conv2'))\n",
        "    model.fc1.register_forward_hook(hook_fn('fc1'))\n",
        "    model.fc2.register_forward_hook(hook_fn('fc2'))\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model for training (using PyTorch's Conv2d)\n",
        "model = MNISTModel(custom_conv=False).float()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, train_loader, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "train_model(model, criterion, optimizer, train_loader, device, num_epochs)\n",
        "\n",
        "# switch to custom conv layers for inference\n",
        "model_custom = MNISTModel(custom_conv=True)\n",
        "model_custom.load_state_dict(model.state_dict())\n",
        "model_custom = model_custom.to(device)\n",
        "\n",
        "# Attach hooks to custom model for printing intermediate tensors\n",
        "attach_hooks(model_custom)\n",
        "\n",
        "# Quantize the model for inference using FP8 (after training is complete)\n",
        "list_exempt_layers = [\"conv1\",\"conv2\",\"fc1\", \"fc2\"]  # Exempt fully connected layers from FP8 conversion\n",
        "model_custom, emulator = mpt_emu.quantize_model(model_custom, dtype=\"E4M3\", list_exempt_layers=list_exempt_layers)\n",
        "\n",
        "# Single image inference with intermediate output printing\n",
        "def inference_with_hooks(model, test_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            single_image = images[0].unsqueeze(0)\n",
        "            print(f\"Input image shape: {single_image.shape}\")\n",
        "            outputs = model(single_image)\n",
        "            print(f\"Model output: {outputs}\")\n",
        "            break\n",
        "\n",
        "inference_with_hooks(model_custom, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m0Qd2i8jo_e",
        "outputId": "9d095edd-b580-4e1e-c1cf-00dd3ef7a420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1319\n",
            "Epoch [2/5], Loss: 0.0411\n",
            "Epoch [3/5], Loss: 0.0291\n",
            "Epoch [4/5], Loss: 0.0210\n",
            "Epoch [5/5], Loss: 0.0160\n",
            "e4m3 : quantizing model weights..\n",
            "Input image shape: torch.Size([1, 1, 28, 28])\n",
            "Layer: conv1\n",
            "Input tensor shape: torch.Size([1, 1, 28, 28])\n",
            "Output tensor shape: torch.Size([1, 32, 28, 28])\n",
            "Input tensor: tensor([[[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,\n",
            "            1.9305,  1.5996,  1.4978,  0.3395,  0.0340, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.4015,\n",
            "            2.8088,  2.8088,  2.8088,  2.8088,  2.6433,  2.0960,  2.0960,\n",
            "            2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  1.7396,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4286,\n",
            "            1.0268,  0.4922,  1.0268,  1.6505,  2.4651,  2.8088,  2.4396,\n",
            "            2.8088,  2.8088,  2.8088,  2.7578,  2.4906,  2.8088,  2.8088,\n",
            "            1.3577, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2078,  0.4159, -0.2460,\n",
            "            0.4286,  0.4286,  0.4286,  0.3268, -0.1569,  2.5797,  2.8088,\n",
            "            0.9250, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242,  0.6322,  2.7960,  2.2360,\n",
            "           -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.1442,  2.5415,  2.8215,  0.6322,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.2177,  2.8088,  2.6051,  0.1358,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242,  0.3268,  2.7451,  2.8088,  0.3649, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242,  1.2686,  2.8088,  1.9560, -0.3606, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.3097,  2.1851,  2.7324,  0.3140, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242,  1.1795,  2.8088,  1.8923, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "            0.5304,  2.7706,  2.6306,  0.3013, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n",
            "            2.3887,  2.8088,  1.6887, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  2.1596,\n",
            "            2.8088,  2.3633,  0.0213, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.8088,\n",
            "            2.8088,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.4269,  2.8088,\n",
            "            1.0395, -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.8088,  2.8088,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  0.3522,  2.6560,  2.8088,  2.8088,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.8088,  2.3633,\n",
            "            0.0849, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.2105, -0.1951,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]])\n",
            "Output tensor: tensor([[[[-0.1806, -0.1789, -0.1789,  ..., -0.1789, -0.1789, -0.2863],\n",
            "          [-0.0831, -0.0114, -0.0114,  ..., -0.0114, -0.0114, -0.0519],\n",
            "          [-0.0831, -0.0114, -0.0114,  ..., -0.0114, -0.0114, -0.0519],\n",
            "          ...,\n",
            "          [-0.0831, -0.0114, -0.0114,  ..., -0.0114, -0.0114, -0.0519],\n",
            "          [-0.0831, -0.0114, -0.0114,  ..., -0.0114, -0.0114, -0.0519],\n",
            "          [-0.0656,  0.0299,  0.0299,  ...,  0.0299,  0.0299,  0.0808]],\n",
            "\n",
            "         [[ 0.0696, -0.1883, -0.1883,  ..., -0.1883, -0.1883, -0.1884],\n",
            "          [ 0.2159,  0.0910,  0.0910,  ...,  0.0910,  0.0910,  0.0715],\n",
            "          [ 0.2159,  0.0910,  0.0910,  ...,  0.0910,  0.0910,  0.0715],\n",
            "          ...,\n",
            "          [ 0.2159,  0.0910,  0.0910,  ...,  0.0910,  0.0910,  0.0715],\n",
            "          [ 0.2159,  0.0910,  0.0910,  ...,  0.0910,  0.0910,  0.0715],\n",
            "          [ 0.4638,  0.4843,  0.4843,  ...,  0.4843,  0.4843,  0.3234]],\n",
            "\n",
            "         [[ 0.4211,  0.4144,  0.4144,  ...,  0.4144,  0.4144,  0.1969],\n",
            "          [ 0.4131,  0.3095,  0.3095,  ...,  0.3095,  0.3095,  0.0341],\n",
            "          [ 0.4131,  0.3095,  0.3095,  ...,  0.3095,  0.3095,  0.0341],\n",
            "          ...,\n",
            "          [ 0.4131,  0.3095,  0.3095,  ...,  0.3095,  0.3095,  0.0341],\n",
            "          [ 0.4131,  0.3095,  0.3095,  ...,  0.3095,  0.3095,  0.0341],\n",
            "          [ 0.2971,  0.1662,  0.1662,  ...,  0.1662,  0.1662,  0.0398]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1413,  0.1229,  0.1229,  ...,  0.1229,  0.1229,  0.1383],\n",
            "          [-0.2680,  0.0076,  0.0076,  ...,  0.0076,  0.0076,  0.0408],\n",
            "          [-0.2680,  0.0076,  0.0076,  ...,  0.0076,  0.0076,  0.0408],\n",
            "          ...,\n",
            "          [-0.2680,  0.0076,  0.0076,  ...,  0.0076,  0.0076,  0.0408],\n",
            "          [-0.2680,  0.0076,  0.0076,  ...,  0.0076,  0.0076,  0.0408],\n",
            "          [-0.1320,  0.0009,  0.0009,  ...,  0.0009,  0.0009, -0.0476]],\n",
            "\n",
            "         [[ 0.0753,  0.4088,  0.4088,  ...,  0.4088,  0.4088,  0.4190],\n",
            "          [-0.2467,  0.0861,  0.0861,  ...,  0.0861,  0.0861,  0.2460],\n",
            "          [-0.2467,  0.0861,  0.0861,  ...,  0.0861,  0.0861,  0.2460],\n",
            "          ...,\n",
            "          [-0.2467,  0.0861,  0.0861,  ...,  0.0861,  0.0861,  0.2460],\n",
            "          [-0.2467,  0.0861,  0.0861,  ...,  0.0861,  0.0861,  0.2460],\n",
            "          [-0.3187, -0.1942, -0.1942,  ..., -0.1942, -0.1942, -0.0258]],\n",
            "\n",
            "         [[-0.3895, -0.4346, -0.4346,  ..., -0.4346, -0.4346, -0.5679],\n",
            "          [-0.4245, -0.5603, -0.5603,  ..., -0.5603, -0.5603, -0.5892],\n",
            "          [-0.4245, -0.5603, -0.5603,  ..., -0.5603, -0.5603, -0.5892],\n",
            "          ...,\n",
            "          [-0.4245, -0.5603, -0.5603,  ..., -0.5603, -0.5603, -0.5892],\n",
            "          [-0.4245, -0.5603, -0.5603,  ..., -0.5603, -0.5603, -0.5892],\n",
            "          [-0.4095, -0.5945, -0.5945,  ..., -0.5945, -0.5945, -0.6125]]]])\n",
            "\n",
            "Layer: conv2\n",
            "Input tensor shape: torch.Size([1, 32, 14, 14])\n",
            "Output tensor shape: torch.Size([1, 64, 14, 14])\n",
            "Input tensor: tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0299, 0.0299, 0.0299,  ..., 0.0299, 0.0299, 0.0808]],\n",
            "\n",
            "         [[0.2159, 0.0910, 0.0910,  ..., 0.0910, 0.0910, 0.0910],\n",
            "          [0.2159, 0.0910, 0.0910,  ..., 0.0910, 0.0910, 0.0910],\n",
            "          [0.2159, 0.0910, 0.0910,  ..., 0.0910, 0.0910, 0.0910],\n",
            "          ...,\n",
            "          [0.2159, 0.0910, 0.0910,  ..., 0.0910, 0.0910, 0.0910],\n",
            "          [0.2159, 0.0910, 0.0910,  ..., 0.0910, 0.0910, 0.0910],\n",
            "          [0.4843, 0.4843, 0.4843,  ..., 0.4843, 0.4843, 0.4843]],\n",
            "\n",
            "         [[0.4211, 0.4144, 0.4144,  ..., 0.4144, 0.4144, 0.4144],\n",
            "          [0.4131, 0.3095, 0.3095,  ..., 0.3095, 0.3095, 0.3095],\n",
            "          [0.4131, 0.3095, 0.3095,  ..., 0.3095, 0.3095, 0.3095],\n",
            "          ...,\n",
            "          [0.4131, 0.3095, 0.3095,  ..., 0.3095, 0.3095, 0.3095],\n",
            "          [0.4131, 0.3095, 0.3095,  ..., 0.3095, 0.3095, 0.3095],\n",
            "          [0.4131, 0.3095, 0.3095,  ..., 0.3095, 0.3095, 0.3095]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1229, 0.1229, 0.1229,  ..., 0.1229, 0.1229, 0.1383],\n",
            "          [0.0076, 0.0076, 0.0076,  ..., 0.0076, 0.0076, 0.0408],\n",
            "          [0.0076, 0.0076, 0.0076,  ..., 0.0076, 0.0076, 0.0408],\n",
            "          ...,\n",
            "          [0.0076, 0.0076, 0.0076,  ..., 0.0076, 0.0076, 0.0408],\n",
            "          [0.0076, 0.0076, 0.0076,  ..., 0.0076, 0.0076, 0.0408],\n",
            "          [0.0076, 0.0076, 0.0076,  ..., 0.0076, 0.0076, 0.0408]],\n",
            "\n",
            "         [[0.4088, 0.4088, 0.4088,  ..., 0.4088, 0.4088, 0.4190],\n",
            "          [0.0861, 0.0861, 0.0861,  ..., 0.0861, 0.0861, 0.2460],\n",
            "          [0.0861, 0.0861, 0.0861,  ..., 0.0861, 0.0861, 0.2460],\n",
            "          ...,\n",
            "          [0.0861, 0.0861, 0.0861,  ..., 0.0861, 0.0861, 0.2460],\n",
            "          [0.0861, 0.0861, 0.0861,  ..., 0.0861, 0.0861, 0.2460],\n",
            "          [0.0861, 0.0861, 0.0861,  ..., 0.0861, 0.0861, 0.2460]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
            "Output tensor: tensor([[[[-0.5664, -0.6275, -0.6609,  ..., -0.6609, -0.7805, -0.4365],\n",
            "          [-0.7494, -0.6656, -0.6513,  ..., -0.6513, -0.7876, -0.5306],\n",
            "          [-0.5128, -0.3995,  0.1991,  ..., -0.0955, -0.4624, -0.3245],\n",
            "          ...,\n",
            "          [-0.5128, -0.3709, -0.3632,  ..., -0.3632, -0.4957, -0.3245],\n",
            "          [-0.4033, -0.2324, -0.2120,  ..., -0.2120, -0.3418, -0.2399],\n",
            "          [-0.3606, -0.2439, -0.1498,  ..., -0.1498, -0.2154, -0.1958]],\n",
            "\n",
            "         [[-0.4467, -0.9608, -0.5871,  ..., -0.5871, -0.6517, -0.0884],\n",
            "          [-0.3498, -0.9180, -0.5184,  ..., -0.5184, -0.5488, -0.2197],\n",
            "          [-0.3632, -1.0647, -0.3390,  ..., -1.2732, -0.5813, -0.1042],\n",
            "          ...,\n",
            "          [-0.3632, -0.8839, -0.5000,  ..., -0.5000, -0.5094, -0.1042],\n",
            "          [-0.4285, -1.0746, -0.6968,  ..., -0.6968, -0.7009, -0.2875],\n",
            "          [-0.1212, -0.4974, -0.3664,  ..., -0.3664, -0.3472, -0.1612]],\n",
            "\n",
            "         [[-1.1617, -1.3218, -1.1515,  ..., -1.1515, -1.1787, -0.5340],\n",
            "          [-1.2000, -1.1985, -0.9830,  ..., -0.9830, -1.0658, -0.4196],\n",
            "          [-1.0899, -1.2612, -0.8990,  ..., -0.8845, -0.9966, -0.3458],\n",
            "          ...,\n",
            "          [-1.0899, -1.1309, -0.9325,  ..., -0.9325, -1.0118, -0.3458],\n",
            "          [-1.1753, -1.2141, -1.0230,  ..., -1.0230, -1.0995, -0.4007],\n",
            "          [-0.7483, -0.9387, -0.8326,  ..., -0.8326, -0.9294, -0.4904]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.4074, -0.8187, -0.5829,  ..., -0.5829, -0.7475, -0.7824],\n",
            "          [-0.8056, -1.4384, -1.0932,  ..., -1.0932, -1.3517, -1.2361],\n",
            "          [-0.5691, -1.5612, -3.2296,  ..., -1.7522, -1.0916, -0.9776],\n",
            "          ...,\n",
            "          [-0.5691, -1.1110, -0.7483,  ..., -0.7483, -0.9944, -0.9776],\n",
            "          [-0.8887, -1.6680, -1.3020,  ..., -1.3020, -1.5495, -1.4009],\n",
            "          [-0.8571, -1.4184, -1.1685,  ..., -1.1685, -1.3217, -1.0661]],\n",
            "\n",
            "         [[-0.2236, -0.1269, -0.1518,  ..., -0.1518, -0.2283,  0.0355],\n",
            "          [-0.5035, -0.6915, -0.6922,  ..., -0.6922, -0.8595, -0.5009],\n",
            "          [-0.3881, -0.5944, -0.4066,  ..., -0.6523, -0.6339, -0.3511],\n",
            "          ...,\n",
            "          [-0.3881, -0.4866, -0.4685,  ..., -0.4685, -0.6404, -0.3511],\n",
            "          [-0.4311, -0.5550, -0.5214,  ..., -0.5214, -0.6896, -0.3945],\n",
            "          [-0.5771, -0.8461, -0.8114,  ..., -0.8114, -0.9235, -0.7161]],\n",
            "\n",
            "         [[-0.9646, -1.1050, -0.9644,  ..., -0.9644, -1.1239, -0.5804],\n",
            "          [-1.1649, -1.3037, -1.0811,  ..., -1.0811, -1.3210, -0.6898],\n",
            "          [-0.9185, -1.5095, -1.7316,  ..., -1.1970, -1.1118, -0.5732],\n",
            "          ...,\n",
            "          [-0.9185, -1.0475, -0.8233,  ..., -0.8233, -1.0844, -0.5732],\n",
            "          [-0.9971, -1.2171, -0.9878,  ..., -0.9878, -1.2474, -0.7213],\n",
            "          [-0.6459, -0.8014, -0.6311,  ..., -0.6311, -0.8149, -0.5097]]]])\n",
            "\n",
            "Layer: fc1\n",
            "Input tensor shape: torch.Size([1, 3136])\n",
            "Output tensor shape: torch.Size([1, 128])\n",
            "Input tensor: tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "Output tensor: tensor([[ -7.9906,  -2.7130,  -2.2516,  -2.4857,   4.0572, -11.5166,   1.5710,\n",
            "          -1.2216, -11.9029,  -1.0101,   6.5811, -13.1464,  -3.6788,  -2.8430,\n",
            "          -2.4277,   1.5828,  -3.2764,  -9.8401,  -4.4084,  -2.8965,  -3.1304,\n",
            "          -8.9145,  -1.5567,  -2.0095,   3.9787,  -3.4422,   0.3490,  -6.9415,\n",
            "          -2.1798,  -1.7622,  -2.3567, -21.3638, -12.1259,   5.4314,   6.0281,\n",
            "          -2.9796,   3.9726,  14.7952,  -1.8378,  -6.6682,   1.5387,  -1.3400,\n",
            "         -13.9761,  -2.0390,  -7.8046,  -8.4775,  -0.2805,  -0.7993,  -1.2410,\n",
            "           9.2148,  -7.7098,  -1.3492,  -2.3617, -14.1238,  -3.4782,  -2.3273,\n",
            "           4.0208,  -8.0619,  -3.0475,  10.3917,  -1.0080,  -1.3163,  -1.9803,\n",
            "         -14.3187,  -3.6119,   9.3148,  -0.9206,   7.3884,   2.2069,  -2.5275,\n",
            "          -4.5601,  -2.3469,  -8.5934,   5.8198,  -3.8516,   6.3414,  -0.9175,\n",
            "          -4.0129,  -1.3122,  -3.7669,  -1.2806,   8.0390,  -0.1612,   8.4189,\n",
            "          -6.7650,  19.1724,   2.9474, -11.4189,   0.1939,  -8.9510,  -5.1088,\n",
            "          -2.6412,   0.4311,  -1.3589,  13.5070,   9.0229,  -2.0661,  -8.5527,\n",
            "          -1.3598,  -3.0048,  12.0880,  -7.0968,  12.4592, -10.8137,  -1.7910,\n",
            "          -2.6540,  -2.4990,  -3.9303,  -2.9735,  -4.4884,  -1.6958,  -2.6529,\n",
            "          -2.1631,  -1.9615,   4.4232,  -1.4573,   1.5946, -17.2522,  -1.5268,\n",
            "         -14.8317,   1.0754,   0.1577,  -2.0842,  -0.3523,  12.1679,  -7.4879,\n",
            "           1.6608,  -6.8617]])\n",
            "\n",
            "Layer: fc2\n",
            "Input tensor shape: torch.Size([1, 128])\n",
            "Output tensor shape: torch.Size([1, 10])\n",
            "Input tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0572,  0.0000,  1.5710,  0.0000,\n",
            "          0.0000,  0.0000,  6.5811,  0.0000,  0.0000,  0.0000,  0.0000,  1.5828,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          3.9787,  0.0000,  0.3490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  5.4314,  6.0281,  0.0000,  3.9726, 14.7952,  0.0000,  0.0000,\n",
            "          1.5387,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  9.2148,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          4.0208,  0.0000,  0.0000, 10.3917,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  9.3148,  0.0000,  7.3884,  2.2069,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  5.8198,  0.0000,  6.3414,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  8.0390,  0.0000,  8.4189,  0.0000, 19.1724,  2.9474,  0.0000,\n",
            "          0.1939,  0.0000,  0.0000,  0.0000,  0.4311,  0.0000, 13.5070,  9.0229,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000, 12.0880,  0.0000, 12.4592,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  4.4232,  0.0000,  1.5946,  0.0000,  0.0000,  0.0000,\n",
            "          1.0754,  0.1577,  0.0000,  0.0000, 12.1679,  0.0000,  1.6608,  0.0000]])\n",
            "Output tensor: tensor([[ -6.6111,  -2.6088,  -1.7343,   2.3980,  -5.8820,  -1.6267, -17.7486,\n",
            "          14.7376,  -1.5507,   0.5425]])\n",
            "\n",
            "Model output: tensor([[ -6.6111,  -2.6088,  -1.7343,   2.3980,  -5.8820,  -1.6267, -17.7486,\n",
            "          14.7376,  -1.5507,   0.5425]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JXRizKDlpT9",
        "outputId": "9426ea31-6b34-4189-8100-aa2b85bc29b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNISTModel(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1WsZKuXls9q",
        "outputId": "86cd2518-77f3-4f0a-d61f-44778920fa80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNISTModel(\n",
            "  (conv1): CustomConv2D()\n",
            "  (conv2): CustomConv2D()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}