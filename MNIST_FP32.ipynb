{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlpRb6jItRz3",
        "outputId": "4daeca60-1609-427c-aafe-aa1d5ca05fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1446\n",
            "Epoch [2/5], Loss: 0.0160\n",
            "Epoch [3/5], Loss: 0.0048\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Accuracy of the model on the test images with custom conv: 98.79%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from Conv2d import CustomConv2D\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, custom_conv=False):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        if custom_conv:\n",
        "            # Use custom conv layer for inference\n",
        "            self.conv1 = CustomConv2D(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = CustomConv2D(32, 64, kernel_size=3, padding=1)\n",
        "        else:\n",
        "            # Use PyTorch's Conv2d for training\n",
        "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model for training (using PyTorch's Conv2d)\n",
        "model = MNISTModel(custom_conv=False).float()  # Use standard Conv2d during training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# switch to custom conv layers for inference\n",
        "model_custom = MNISTModel(custom_conv=True).float()  # Use custom convolution layer for inference\n",
        "model_custom.load_state_dict(model.state_dict())  # Load the trained weights\n",
        "\n",
        "# Testing with the custom convolution layer\n",
        "model_custom.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model_custom(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images with custom conv: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOkBe_AMtXIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd6ca84-5bd2-4213-d8c4-5193aa794aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNISTModel(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
            "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
            "            Linear-5                  [-1, 128]         401,536\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.36\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 1.97\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "print(model)\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from Conv2d import CustomConv2D\n",
        "\n",
        "# Define a hook function to print input and output tensors\n",
        "def forward_hook(module, input, output):\n",
        "    print(f\"Layer: {module.__class__.__name__}\")\n",
        "    print(\"Input Tensor:\")\n",
        "    print(input[0])  # Print the input tensor\n",
        "    print(\"Output Tensor:\")\n",
        "    print(output)    # Print the output tensor\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, custom_conv=False):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        if custom_conv:\n",
        "            # Use custom conv layer for inference\n",
        "            self.conv1 = CustomConv2D(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = CustomConv2D(32, 64, kernel_size=3, padding=1)\n",
        "        else:\n",
        "            # Use PyTorch's Conv2d for training\n",
        "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model for training (using PyTorch's Conv2d)\n",
        "model = MNISTModel(custom_conv=False).float()  # Use standard Conv2d during training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# switch to custom conv layers for inference\n",
        "model_custom = MNISTModel(custom_conv=True).float()  # Use custom convolution layer for inference\n",
        "model_custom.load_state_dict(model.state_dict())  # Load the trained weights\n",
        "\n",
        "# Register forward hooks on the layers you want to monitor\n",
        "model_custom.conv1.register_forward_hook(forward_hook)\n",
        "model_custom.conv2.register_forward_hook(forward_hook)\n",
        "model_custom.fc1.register_forward_hook(forward_hook)\n",
        "model_custom.fc2.register_forward_hook(forward_hook)\n",
        "\n",
        "# Testing with the custom convolution layer on a single image\n",
        "model_custom.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a single batch of images and labels\n",
        "    for images, labels in test_loader:\n",
        "        # Use only the first image for this example\n",
        "        single_image = images[0].unsqueeze(0)\n",
        "        outputs = model_custom(single_image)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        print(f\"Predicted label: {predicted.item()}\")\n",
        "        break  # We only need a single image for inference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3diTb526qVFy",
        "outputId": "a41c8ec7-a158-4d8f-c42f-6d0d197abb54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0268\n",
            "Epoch [2/5], Loss: 0.0069\n",
            "Epoch [3/5], Loss: 0.0005\n",
            "Epoch [4/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.1129\n",
            "Layer: CustomConv2D\n",
            "Input Tensor:\n",
            "tensor([[[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,\n",
            "            1.9305,  1.5996,  1.4978,  0.3395,  0.0340, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.4015,\n",
            "            2.8088,  2.8088,  2.8088,  2.8088,  2.6433,  2.0960,  2.0960,\n",
            "            2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  1.7396,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4286,\n",
            "            1.0268,  0.4922,  1.0268,  1.6505,  2.4651,  2.8088,  2.4396,\n",
            "            2.8088,  2.8088,  2.8088,  2.7578,  2.4906,  2.8088,  2.8088,\n",
            "            1.3577, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2078,  0.4159, -0.2460,\n",
            "            0.4286,  0.4286,  0.4286,  0.3268, -0.1569,  2.5797,  2.8088,\n",
            "            0.9250, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242,  0.6322,  2.7960,  2.2360,\n",
            "           -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.1442,  2.5415,  2.8215,  0.6322,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.2177,  2.8088,  2.6051,  0.1358,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242,  0.3268,  2.7451,  2.8088,  0.3649, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242,  1.2686,  2.8088,  1.9560, -0.3606, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.3097,  2.1851,  2.7324,  0.3140, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242,  1.1795,  2.8088,  1.8923, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "            0.5304,  2.7706,  2.6306,  0.3013, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n",
            "            2.3887,  2.8088,  1.6887, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  2.1596,\n",
            "            2.8088,  2.3633,  0.0213, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.8088,\n",
            "            2.8088,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.4269,  2.8088,\n",
            "            1.0395, -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.8088,  2.8088,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  0.3522,  2.6560,  2.8088,  2.8088,\n",
            "            0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.8088,  2.3633,\n",
            "            0.0849, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.2105, -0.1951,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
            "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]])\n",
            "Output Tensor:\n",
            "tensor([[[[ 0.3182,  0.4481,  0.4481,  ...,  0.4481,  0.4481,  0.3042],\n",
            "          [ 0.3421,  0.4329,  0.4329,  ...,  0.4329,  0.4329,  0.1770],\n",
            "          [ 0.3421,  0.4329,  0.4329,  ...,  0.4329,  0.4329,  0.1770],\n",
            "          ...,\n",
            "          [ 0.3421,  0.4329,  0.4329,  ...,  0.4329,  0.4329,  0.1770],\n",
            "          [ 0.3421,  0.4329,  0.4329,  ...,  0.4329,  0.4329,  0.1770],\n",
            "          [ 0.1795,  0.2284,  0.2284,  ...,  0.2284,  0.2284,  0.0054]],\n",
            "\n",
            "         [[ 0.3206,  0.4869,  0.4869,  ...,  0.4869,  0.4869,  0.3657],\n",
            "          [ 0.0248,  0.2420,  0.2420,  ...,  0.2420,  0.2420,  0.2511],\n",
            "          [ 0.0248,  0.2420,  0.2420,  ...,  0.2420,  0.2420,  0.2511],\n",
            "          ...,\n",
            "          [ 0.0248,  0.2420,  0.2420,  ...,  0.2420,  0.2420,  0.2511],\n",
            "          [ 0.0248,  0.2420,  0.2420,  ...,  0.2420,  0.2420,  0.2511],\n",
            "          [-0.1405,  0.0048,  0.0048,  ...,  0.0048,  0.0048,  0.1313]],\n",
            "\n",
            "         [[-0.0015, -0.0910, -0.0910,  ..., -0.0910, -0.0910, -0.3407],\n",
            "          [ 0.0668, -0.0060, -0.0060,  ..., -0.0060, -0.0060, -0.2684],\n",
            "          [ 0.0668, -0.0060, -0.0060,  ..., -0.0060, -0.0060, -0.2684],\n",
            "          ...,\n",
            "          [ 0.0668, -0.0060, -0.0060,  ..., -0.0060, -0.0060, -0.2684],\n",
            "          [ 0.0668, -0.0060, -0.0060,  ..., -0.0060, -0.0060, -0.2684],\n",
            "          [ 0.1929,  0.2931,  0.2931,  ...,  0.2931,  0.2931,  0.0781]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3817,  0.2815,  0.2815,  ...,  0.2815,  0.2815, -0.0684],\n",
            "          [ 0.2546, -0.0248, -0.0248,  ..., -0.0248, -0.0248, -0.3684],\n",
            "          [ 0.2546, -0.0248, -0.0248,  ..., -0.0248, -0.0248, -0.3684],\n",
            "          ...,\n",
            "          [ 0.2546, -0.0248, -0.0248,  ..., -0.0248, -0.0248, -0.3684],\n",
            "          [ 0.2546, -0.0248, -0.0248,  ..., -0.0248, -0.0248, -0.3684],\n",
            "          [-0.1044, -0.4224, -0.4224,  ..., -0.4224, -0.4224, -0.6410]],\n",
            "\n",
            "         [[-0.4783, -0.2332, -0.2332,  ..., -0.2332, -0.2332, -0.0595],\n",
            "          [-0.5398, -0.3092, -0.3092,  ..., -0.3092, -0.3092, -0.1485],\n",
            "          [-0.5398, -0.3092, -0.3092,  ..., -0.3092, -0.3092, -0.1485],\n",
            "          ...,\n",
            "          [-0.5398, -0.3092, -0.3092,  ..., -0.3092, -0.3092, -0.1485],\n",
            "          [-0.5398, -0.3092, -0.3092,  ..., -0.3092, -0.3092, -0.1485],\n",
            "          [-0.6129, -0.5190, -0.5190,  ..., -0.5190, -0.5190, -0.3739]],\n",
            "\n",
            "         [[-0.2998, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.1134],\n",
            "          [-0.1133, -0.1560, -0.1560,  ..., -0.1560, -0.1560, -0.0251],\n",
            "          [-0.1133, -0.1560, -0.1560,  ..., -0.1560, -0.1560, -0.0251],\n",
            "          ...,\n",
            "          [-0.1133, -0.1560, -0.1560,  ..., -0.1560, -0.1560, -0.0251],\n",
            "          [-0.1133, -0.1560, -0.1560,  ..., -0.1560, -0.1560, -0.0251],\n",
            "          [-0.1222, -0.0864, -0.0864,  ..., -0.0864, -0.0864, -0.0237]]]])\n",
            "--------------------------------------------------\n",
            "Layer: CustomConv2D\n",
            "Input Tensor:\n",
            "tensor([[[[0.4481, 0.4481, 0.4481,  ..., 0.4481, 0.4481, 0.4481],\n",
            "          [0.4329, 0.4329, 0.4329,  ..., 0.4329, 0.4329, 0.4329],\n",
            "          [0.4329, 0.4329, 0.4329,  ..., 0.4329, 0.4329, 0.4329],\n",
            "          ...,\n",
            "          [0.4329, 0.4329, 0.4329,  ..., 0.4329, 0.4329, 0.4329],\n",
            "          [0.4329, 0.4329, 0.4329,  ..., 0.4329, 0.4329, 0.4329],\n",
            "          [0.4329, 0.4329, 0.4329,  ..., 0.4329, 0.4329, 0.4329]],\n",
            "\n",
            "         [[0.4869, 0.4869, 0.4869,  ..., 0.4869, 0.4869, 0.4869],\n",
            "          [0.2420, 0.2420, 0.2420,  ..., 0.2420, 0.2420, 0.2511],\n",
            "          [0.2420, 0.2420, 0.2420,  ..., 0.2420, 0.2420, 0.2511],\n",
            "          ...,\n",
            "          [0.2420, 0.2420, 0.2420,  ..., 0.2420, 0.2420, 0.2511],\n",
            "          [0.2420, 0.2420, 0.2420,  ..., 0.2420, 0.2420, 0.2511],\n",
            "          [0.2420, 0.2420, 0.2420,  ..., 0.2420, 0.2420, 0.2511]],\n",
            "\n",
            "         [[0.0668, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0668, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0668, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0668, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0668, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2931, 0.2931, 0.2931,  ..., 0.2931, 0.2931, 0.2931]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.3817, 0.2815, 0.2815,  ..., 0.2815, 0.2815, 0.2815],\n",
            "          [0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
            "Output Tensor:\n",
            "tensor([[[[-0.9133, -0.9377, -0.8696,  ..., -0.8696, -0.9091, -0.4129],\n",
            "          [-1.3312, -1.7058, -1.6418,  ..., -1.6418, -1.6648, -1.0387],\n",
            "          [-1.2410, -1.3895, -0.3197,  ..., -2.6063, -1.6359, -0.9314],\n",
            "          ...,\n",
            "          [-1.2410, -1.5850, -1.5294,  ..., -1.5294, -1.5577, -0.9314],\n",
            "          [-1.2305, -1.7481, -1.7026,  ..., -1.7026, -1.7079, -1.2344],\n",
            "          [-0.6700, -1.2976, -1.2799,  ..., -1.2799, -1.2874, -1.1321]],\n",
            "\n",
            "         [[-1.1938, -2.1792, -2.0534,  ..., -2.0534, -2.1941, -2.0929],\n",
            "          [-1.4084, -2.5809, -2.3938,  ..., -2.3938, -2.5974, -2.3329],\n",
            "          [-1.2392, -2.4683, -3.5258,  ..., -3.5313, -2.3893, -2.0188],\n",
            "          ...,\n",
            "          [-1.2392, -2.2540, -2.0346,  ..., -2.0346, -2.2511, -2.0188],\n",
            "          [-1.5497, -2.8346, -2.6348,  ..., -2.6348, -2.8539, -2.5028],\n",
            "          [-0.9413, -1.6509, -1.5120,  ..., -1.5120, -1.6519, -1.3871]],\n",
            "\n",
            "         [[-1.1456, -1.5346, -1.5569,  ..., -1.5569, -1.6721, -1.0953],\n",
            "          [-1.4971, -1.9821, -2.0438,  ..., -2.0438, -2.1706, -1.4001],\n",
            "          [-1.3447, -2.1146, -2.8885,  ..., -1.0760, -1.9203, -1.2746],\n",
            "          ...,\n",
            "          [-1.3447, -1.7520, -1.8245,  ..., -1.8245, -1.9644, -1.2746],\n",
            "          [-1.4697, -1.7372, -1.8106,  ..., -1.8106, -1.9439, -1.1991],\n",
            "          [-0.9771, -0.9692, -1.0426,  ..., -1.0426, -1.1125, -0.6218]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.4583, -0.6572, -0.5766,  ..., -0.5766, -0.5992, -0.3934],\n",
            "          [-1.2340, -1.7562, -1.6674,  ..., -1.6674, -1.6770, -0.9734],\n",
            "          [-1.0924, -1.0287, -0.6903,  ..., -1.5975, -1.5192, -0.9054],\n",
            "          ...,\n",
            "          [-1.0924, -1.5748, -1.4977,  ..., -1.4977, -1.5148, -0.9054],\n",
            "          [-1.0431, -1.5398, -1.4578,  ..., -1.4578, -1.5044, -0.9381],\n",
            "          [-1.3508, -1.9204, -1.8810,  ..., -1.8810, -1.9358, -1.1317]],\n",
            "\n",
            "         [[-0.7614, -0.9044, -0.7591,  ..., -0.7591, -0.8878, -0.5302],\n",
            "          [-1.0285, -1.3184, -1.1944,  ..., -1.1944, -1.3287, -0.9062],\n",
            "          [-1.0109, -1.8234, -1.9826,  ..., -2.4393, -1.4210, -0.8849],\n",
            "          ...,\n",
            "          [-1.0109, -1.3027, -1.1739,  ..., -1.1739, -1.3120, -0.8849],\n",
            "          [-1.1467, -1.6336, -1.5196,  ..., -1.5196, -1.6374, -1.1842],\n",
            "          [-0.6312, -1.0007, -0.9855,  ..., -0.9855, -1.0145, -0.8198]],\n",
            "\n",
            "         [[-1.2444, -1.4053, -1.2965,  ..., -1.2965, -1.4208, -0.7634],\n",
            "          [-1.4684, -1.9289, -1.8095,  ..., -1.8095, -1.9563, -1.2171],\n",
            "          [-1.2657, -2.0316, -2.2491,  ..., -2.4069, -1.7267, -0.9985],\n",
            "          ...,\n",
            "          [-1.2657, -1.5967, -1.4777,  ..., -1.4777, -1.6403, -0.9985],\n",
            "          [-1.4055, -1.8673, -1.7608,  ..., -1.7608, -1.9000, -1.2159],\n",
            "          [-0.9284, -1.3777, -1.3458,  ..., -1.3458, -1.4108, -1.0461]]]])\n",
            "--------------------------------------------------\n",
            "Layer: Linear\n",
            "Input Tensor:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "Output Tensor:\n",
            "tensor([[ -1.2810,  -1.1529,  -7.5654,   6.1487,  -3.9581,  -4.0012,  -2.0853,\n",
            "         -12.0280,   2.4695, -11.8123,  -9.9374,  -1.8870,  -2.9388,  -6.2476,\n",
            "          -2.2813,   4.5787,  -4.1827,  -2.9252,  -2.8339,   1.8467,   4.3878,\n",
            "           2.4888, -16.4583,  -5.9161,  10.3381,  -9.8317, -15.4509,   3.6346,\n",
            "          -7.9475,  -2.1992, -13.5777,  -1.9550,  -2.3974,  -3.3231,  -3.6648,\n",
            "          -2.4483,   0.3254,  -2.0463, -24.3802,  -4.3945,   1.6981,  -1.6537,\n",
            "          -5.8683,  10.8990,  -6.6163,   3.9854,  -2.1207,  14.4043,  -1.8521,\n",
            "          -5.5177,  -2.1671,  -9.3975,   2.4553,   2.4206,  -1.0135,  -5.1185,\n",
            "          -2.3820, -10.3230,   6.7215,   9.7660,  -3.0159,  -2.0022, -12.1159,\n",
            "          -2.2745,  -2.2647,  -2.8969,   1.2793,  -6.1337,   7.1559, -10.2242,\n",
            "          -1.7545,  -2.1428,   0.9789,  -0.8189,  14.9888,  -1.7143,   5.7522,\n",
            "           5.6199,  -3.6327,  -2.5572, -17.5254,  -5.5182,  -0.5307,  -7.5483,\n",
            "           7.5642,  -5.5310,  -0.1430,  -9.1461, -16.9702,   9.0149, -11.1416,\n",
            "          -6.5035,   0.8447, -13.0851, -10.0281,  -1.5617,  -3.7089,   0.7842,\n",
            "          11.1585,  -4.9110,  -2.6371,  -3.0279,   8.9488,  -1.8474,  -2.1418,\n",
            "          -2.5545,  -5.6681,   4.7200,  -7.9966,   2.7674,  -3.4823, -10.8672,\n",
            "           2.2921,  -4.8165,   9.4780,   7.8349,   8.0180,  -1.9274,  -1.8792,\n",
            "           1.5824,  -2.6654,  -1.5126,  -8.4867,  -2.0674,   2.8876,  17.1033,\n",
            "          -1.7036,  -1.9118]])\n",
            "--------------------------------------------------\n",
            "Layer: Linear\n",
            "Input Tensor:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  6.1487,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          2.4695,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.5787,\n",
            "          0.0000,  0.0000,  0.0000,  1.8467,  4.3878,  2.4888,  0.0000,  0.0000,\n",
            "         10.3381,  0.0000,  0.0000,  3.6346,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.3254,  0.0000,  0.0000,  0.0000,\n",
            "          1.6981,  0.0000,  0.0000, 10.8990,  0.0000,  3.9854,  0.0000, 14.4043,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  2.4553,  2.4206,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  6.7215,  9.7660,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  1.2793,  0.0000,  7.1559,  0.0000,  0.0000,  0.0000,\n",
            "          0.9789,  0.0000, 14.9888,  0.0000,  5.7522,  5.6199,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  7.5642,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  9.0149,  0.0000,  0.0000,  0.8447,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.7842, 11.1585,  0.0000,  0.0000,  0.0000,  8.9488,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  4.7200,  0.0000,  2.7674,  0.0000,  0.0000,\n",
            "          2.2921,  0.0000,  9.4780,  7.8349,  8.0180,  0.0000,  0.0000,  1.5824,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  2.8876, 17.1033,  0.0000,  0.0000]])\n",
            "Output Tensor:\n",
            "tensor([[ -3.3577,  -3.0786,   0.8020,  -2.0239,  -5.3463, -11.4973, -13.3832,\n",
            "          15.7506,  -2.8940,   1.1620]])\n",
            "--------------------------------------------------\n",
            "Predicted label: 7\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}